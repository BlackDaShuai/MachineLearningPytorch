{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-07T08:39:36.364096Z",
     "start_time": "2024-10-07T08:39:12.573820Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torch import nn, optim\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 载入训练集数据，同时把数据转换为tensor，同时下载数据\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# 载入测试集数据，同时把数据转换为tensor，同时下载数据\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# 文件分别为训练集测试集他们分别的数据和标签\n",
    "# 批次大小\n",
    "batch_size = 64\n",
    "# 装载数据集,dataloader为数据的装载器，数据来源于dataset=train_dataset, 大小为batch_size=batch_size，方式为随机打乱\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=8)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=8)\n",
    "# 展示数据\n",
    "for i, data in enumerate(train_loader):\n",
    "    inputs, labels = data\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "Dop = 0.2  # 0.1的神经元不工作\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    # 初始化，定义网络结构\n",
    "    def __init__(self):\n",
    "        # 初始化nnModule\n",
    "        super(LSTM, self).__init__()\n",
    "        # input：输入特征的大小\n",
    "        # hidden：LSTM模块的数量\n",
    "        # layers：隐藏层的层数\n",
    "        # lstm默认input(seq_len, batch, feature)\n",
    "        # batchfirst后，变成(feature, seq_len, batch)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=28,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    # 前向计算，定义网络计算\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28, 28)\n",
    "        # 三个输出\n",
    "        # output:[batch,seq_len,hidden_size]，包含每个序列的输出结果\n",
    "        # seq_len取27时，就是最后一个结果\n",
    "        # 虽然LSTW的batch_first为True，但是h_n,c_n的第0个维度还是numlayers\n",
    "        # h_n:[num_layers, batch, hidden_size]，只包含最后一个序列的输出结果hidden\n",
    "        # c_n:[num_layers, batch, hidden_size]，只包含最后一个序列的输出结果cell\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        #output_last = self.fc(output[:, -1, :])  #只要最后一个结果\n",
    "        output_last = h_n[-1, :, :]  #只要最后一个结果\n",
    "        x = self.fc(output_last)\n",
    "        outs = self.softmax(x)\n",
    "\n",
    "        return outs\n",
    "\n",
    "\n",
    "LR = 0.001\n",
    "# 定义模型\n",
    "model = LSTM()\n",
    "\n",
    "# 载入模型参数\n",
    "model.load_state_dict(torch.load('modle/LSTMnum.pth'))\n",
    "# 定义损失函数,交叉熵损失\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    # 测试状态\n",
    "    model.eval()\n",
    "    # 计算测试集的准确率\n",
    "    correct = 0\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        out = model(inputs)\n",
    "        # 计算out中，最大值所在位置\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "\n",
    "        correct += (predicted == labels).sum()\n",
    "    print(\"Test acc:{0}\".format(correct.item() / len(test_loader.dataset)))\n",
    "\n",
    "    # 计算训练集的准确率\n",
    "    correct = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        out = model(inputs)\n",
    "        # 计算out中，最大值所在位置\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "\n",
    "        correct += (predicted == labels).sum()\n",
    "    print(\"Train acc:{0}\".format(correct.item() / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "    # 不再训练\n",
    "    # train()\n",
    "\n",
    "\n",
    "test()\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BLKDASH\\AppData\\Local\\Temp\\ipykernel_17576\\1526851729.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('modle/LSTMnum.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc:0.9638\n",
      "Train acc:0.9645333333333334\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
